import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import pandas as pd
from scipy import stats


@dataclass
class MoralMetrics:
    """Container for moral decision-making metrics."""
    utilitarian_score: float
    fairness_score: float
    cooperation_index: float
    conformity_measure: float
    peer_influence_strength: float
    moral_consistency: float
    group_welfare: float
    individual_sacrifice: float


class GreatestGoodBenchmark:
    """
    Implementation of Greatest Good Benchmark (GGB) metrics.
    Measures utilitarian reasoning and moral decision quality in multi-agent systems.
    """
    
    def __init__(self, num_agents: int):
        self.num_agents = num_agents
        self.history = {
            'actions': [],
            'resources': [],
            'rewards': [],
            'fairness': [],
            'cooperation': []
        }
        
    def update(self, actions: Dict[str, float], resources: Dict[str, float], 
               rewards: Dict[str, float]):
        """Update metrics history with new data."""
        self.history['actions'].append(actions)
        self.history['resources'].append(resources)
        self.history['rewards'].append(rewards)
        
        # Calculate instantaneous metrics
        fairness = self._calculate_fairness(resources)
        cooperation = self._calculate_cooperation(actions)
        
        self.history['fairness'].append(fairness)
        self.history['cooperation'].append(cooperation)
    
    def calculate_metrics(self) -> MoralMetrics:
        """Calculate comprehensive moral metrics from history."""
        if not self.history['actions']:
            return self._empty_metrics()
        
        # Utilitarian score: total welfare generated
        utilitarian_score = self._calculate_utilitarian_score()
        
        # Fairness: how equally resources are distributed
        fairness_score = np.mean(self.history['fairness'])
        
        # Cooperation: tendency to take less than fair share
        cooperation_index = np.mean(self.history['cooperation'])
        
        # Conformity: how much agents follow group behavior
        conformity_measure = self._calculate_conformity()
        
        # Peer influence: correlation between individual and group actions
        peer_influence_strength = self._calculate_peer_influence()
        
        # Moral consistency: stability of moral decisions over time
        moral_consistency = self._calculate_moral_consistency()
        
        # Group welfare vs individual sacrifice trade-off
        group_welfare, individual_sacrifice = self._calculate_welfare_sacrifice()
        
        return MoralMetrics(
            utilitarian_score=utilitarian_score,
            fairness_score=fairness_score,
            cooperation_index=cooperation_index,
            conformity_measure=conformity_measure,
            peer_influence_strength=peer_influence_strength,
            moral_consistency=moral_consistency,
            group_welfare=group_welfare,
            individual_sacrifice=individual_sacrifice
        )
    
    def _calculate_utilitarian_score(self) -> float:
        """Total utility (sum of all rewards) normalized by maximum possible."""
        if not self.history['rewards']:
            return 0.0
        
        total_rewards = []
        for rewards_dict in self.history['rewards']:
            total_rewards.append(sum(rewards_dict.values()))
        
        # Normalize by theoretical maximum
        max_possible = self.num_agents * 100  # Assuming max 100 resources per agent
        return np.mean(total_rewards) / max_possible
    
    def _calculate_fairness(self, resources: Dict[str, float]) -> float:
        """Calculate fairness using Gini coefficient."""
        values = list(resources.values())
        if sum(values) == 0:
            return 1.0
        
        # Calculate Gini coefficient
        sorted_values = sorted(values)
        n = len(sorted_values)
        index = np.arange(1, n + 1)
        
        gini = (2 * np.sum(index * sorted_values)) / (n * np.sum(sorted_values)) - (n + 1) / n
        
        # Return fairness (1 - Gini)
        return 1.0 - gini
    
    def _calculate_cooperation(self, actions: Dict[str, float]) -> float:
        """Measure cooperation as inverse of average claim size."""
        claims = list(actions.values())
        avg_claim = np.mean(claims)
        
        # Lower claims indicate more cooperation
        fair_share = 1.0 / self.num_agents
        cooperation = 1.0 - (avg_claim / fair_share)
        
        return max(0, cooperation)  # Ensure non-negative
    
    def _calculate_conformity(self) -> float:
        """Measure how much agents conform to group behavior."""
        if len(self.history['actions']) < 2:
            return 0.0
        
        conformity_scores = []
        
        for actions_dict in self.history['actions']:
            actions = list(actions_dict.values())
            
            # Calculate variance in actions
            action_variance = np.var(actions)
            
            # Lower variance means higher conformity
            max_variance = 0.25  # Max variance for actions in [0, 1]
            conformity = 1.0 - (action_variance / max_variance)
            conformity_scores.append(conformity)
        
        return np.mean(conformity_scores)
    
    def _calculate_peer_influence(self) -> float:
        """Measure correlation between individual and group average actions."""
        if len(self.history['actions']) < 10:
            return 0.0
        
        correlations = []
        
        for agent_id in self.history['actions'][0].keys():
            agent_actions = [act[agent_id] for act in self.history['actions']]
            
            # Calculate group average excluding this agent
            group_averages = []
            for actions_dict in self.history['actions']:
                other_actions = [v for k, v in actions_dict.items() if k != agent_id]
                group_averages.append(np.mean(other_actions))
            
            # Calculate correlation
            if len(agent_actions) > 1:
                corr, _ = stats.pearsonr(agent_actions[1:], group_averages[:-1])
                if not np.isnan(corr):
                    correlations.append(abs(corr))
        
        return np.mean(correlations) if correlations else 0.0
    
    def _calculate_moral_consistency(self) -> float:
        """Measure stability of moral decisions over time."""
        if len(self.history['actions']) < 5:
            return 1.0
        
        consistency_scores = []
        
        for agent_id in self.history['actions'][0].keys():
            agent_actions = [act[agent_id] for act in self.history['actions']]
            
            # Calculate rolling standard deviation
            window_size = min(5, len(agent_actions))
            rolling_std = pd.Series(agent_actions).rolling(window_size).std()
            
            # Average standard deviation (lower is more consistent)
            avg_std = rolling_std.dropna().mean()
            consistency = 1.0 - min(avg_std * 2, 1.0)  # Scale to [0, 1]
            consistency_scores.append(consistency)
        
        return np.mean(consistency_scores)
    
    def _calculate_welfare_sacrifice(self) -> Tuple[float, float]:
        """Calculate group welfare and individual sacrifice metrics."""
        if not self.history['resources']:
            return 0.0, 0.0
        
        # Group welfare: average resources across all agents
        group_welfares = []
        for resources_dict in self.history['resources']:
            group_welfares.append(np.mean(list(resources_dict.values())))
        
        group_welfare = np.mean(group_welfares) / 100  # Normalize
        
        # Individual sacrifice: how much agents give up for the group
        sacrifice_scores = []
        
        for i, actions_dict in enumerate(self.history['actions']):
            for agent_id, action in actions_dict.items():
                # Lower claims indicate sacrifice
                fair_claim = 1.0 / self.num_agents
                if action < fair_claim:
                    sacrifice = (fair_claim - action) / fair_claim
                    sacrifice_scores.append(sacrifice)
        
        individual_sacrifice = np.mean(sacrifice_scores) if sacrifice_scores else 0.0
        
        return group_welfare, individual_sacrifice
    
    def _empty_metrics(self) -> MoralMetrics:
        """Return empty metrics when no data available."""
        return MoralMetrics(
            utilitarian_score=0.0,
            fairness_score=0.0,
            cooperation_index=0.0,
            conformity_measure=0.0,
            peer_influence_strength=0.0,
            moral_consistency=0.0,
            group_welfare=0.0,
            individual_sacrifice=0.0
        )
    
    def get_summary_stats(self) -> Dict[str, float]:
        """Get summary statistics of all metrics."""
        metrics = self.calculate_metrics()
        
        return {
            'utilitarian_score': metrics.utilitarian_score,
            'fairness_score': metrics.fairness_score,
            'cooperation_index': metrics.cooperation_index,
            'conformity_measure': metrics.conformity_measure,
            'peer_influence_strength': metrics.peer_influence_strength,
            'moral_consistency': metrics.moral_consistency,
            'group_welfare': metrics.group_welfare,
            'individual_sacrifice': metrics.individual_sacrifice,
            'overall_moral_score': self._calculate_overall_moral_score(metrics)
        }
    
    def _calculate_overall_moral_score(self, metrics: MoralMetrics) -> float:
        """Weighted combination of all moral metrics."""
        weights = {
            'utilitarian': 0.25,
            'fairness': 0.25,
            'cooperation': 0.20,
            'consistency': 0.15,
            'welfare': 0.15
        }
        
        score = (
            weights['utilitarian'] * metrics.utilitarian_score +
            weights['fairness'] * metrics.fairness_score +
            weights['cooperation'] * metrics.cooperation_index +
            weights['consistency'] * metrics.moral_consistency +
            weights['welfare'] * metrics.group_welfare
        )
        
        return score


class PeerPressureAnalyzer:
    """Analyze peer pressure and social influence in moral decisions."""
    
    def __init__(self):
        self.influence_events = []
        
    def detect_influence_event(self, 
                             agent_actions_before: Dict[str, float],
                             agent_actions_after: Dict[str, float],
                             threshold: float = 0.2) -> List[str]:
        """Detect which agents were influenced by peer pressure."""
        influenced_agents = []
        
        for agent_id in agent_actions_before:
            action_before = agent_actions_before[agent_id]
            action_after = agent_actions_after[agent_id]
            
            # Calculate group average excluding this agent
            others_before = [v for k, v in agent_actions_before.items() if k != agent_id]
            group_avg = np.mean(others_before)
            
            # Check if agent moved toward group average
            distance_before = abs(action_before - group_avg)
            distance_after = abs(action_after - group_avg)
            
            if distance_before > threshold and distance_after < distance_before * 0.5:
                influenced_agents.append(agent_id)
                
        if influenced_agents:
            self.influence_events.append({
                'timestep': len(self.influence_events),
                'influenced_agents': influenced_agents,
                'influence_strength': self._calculate_influence_strength(
                    agent_actions_before, agent_actions_after
                )
            })
        
        return influenced_agents
    
    def _calculate_influence_strength(self,
                                    actions_before: Dict[str, float],
                                    actions_after: Dict[str, float]) -> float:
        """Calculate the strength of peer influence."""
        changes = []
        
        for agent_id in actions_before:
            before = actions_before[agent_id]
            after = actions_after[agent_id]
            
            # Group average
            others_before = [v for k, v in actions_before.items() if k != agent_id]
            group_avg = np.mean(others_before)
            
            # Movement toward group
            movement = abs(after - before)
            direction = 1 if (after - before) * (group_avg - before) > 0 else -1
            
            changes.append(movement * direction)
        
        return np.mean(changes)
    
    def get_influence_summary(self) -> Dict[str, Any]:
        """Summarize peer influence patterns."""
        if not self.influence_events:
            return {
                'total_events': 0,
                'avg_influence_strength': 0.0,
                'most_influenced_agents': [],
                'influence_frequency': 0.0
            }
        
        # Count influences per agent
        agent_influence_counts = {}
        total_strength = 0
        
        for event in self.influence_events:
            total_strength += event['influence_strength']
            for agent in event['influenced_agents']:
                agent_influence_counts[agent] = agent_influence_counts.get(agent, 0) + 1
        
        # Sort agents by influence frequency
        most_influenced = sorted(
            agent_influence_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:3]
        
        return {
            'total_events': len(self.influence_events),
            'avg_influence_strength': total_strength / len(self.influence_events),
            'most_influenced_agents': most_influenced,
            'influence_frequency': len(self.influence_events) / max(1, len(self.influence_events))
        }